{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ed1f3c",
   "metadata": {},
   "source": [
    "# Lab 3: Extending Logistic Regression\n",
    "\n",
    "Author: Ephraim Sun (Masters), Jadon Swearingen (Undergrad), Adeeb Abdul Taher (Masters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13680cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn termcolor plotly imblearn squarify pandas matplotlib seabor matplotlib-ven tqd missingno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8bab0",
   "metadata": {},
   "source": [
    "Here, we import most of the librarys useful for this sort of data analysis. We looked at the file that came with the data set to look at potential imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data processing & analysis packages.\n",
    "\n",
    "## Data Processing.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "## Data Visualisation.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "## Machine Learning.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,accuracy_score,classification_report\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import feature_selection\n",
    "import squarify\n",
    "# import imblearn\n",
    "\n",
    "## Warning indication.\n",
    "import warnings\n",
    "warnings.filterwarnings('always') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242c93b",
   "metadata": {},
   "source": [
    "# Preparation and Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d7f04",
   "metadata": {},
   "source": [
    "The purpose of the Income Prediction DataSet provided by Zindi is to analyze and classify if a person earns more or less than \\$50,000 based on various data variables. Specifically, we are given a random population of over 100,000 individuals to test on 200,000 training data. The data is collected from a random American population where the media income was about $50,000 and is provided by the company Zindi. Third parties like Zindi are interested because they will use the machine learning algorithm to classify further individuals and people, giving them a score based on these model accuracy. Furthermore, third parties can use this data in applications like Finance (credit scoring and loans, fraud detection), Healthcare (health insurance and healthcare programs), as well as other areas like HR and Public Policy. The current predicition algorithm that will be considered useful to other parties is that it must be better than 96%, which another person has achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54633d03",
   "metadata": {},
   "source": [
    "Here, we make sure that the data is read correctly, as well as see what a few rows look like and describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5191bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets.\n",
    "\n",
    "df = pd.read_csv(\"Train.csv\")\n",
    "\n",
    "# Display the first 5 rows of the train dataset.\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbf325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print('===========')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a4fbb",
   "metadata": {},
   "source": [
    "Here, we start to make some initial graphs. This is adapted from the slideshow from eric larson. We look at all of the data, seeing which are null and sort it by class and age, which are two values that we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this python magics will allow plot to be embedded into the notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "\n",
    "# plt.subplots(figsize=(20, 15))\n",
    "# External package: conda install missingno \n",
    "import missingno as mn\n",
    "\n",
    "mn.matrix(df)\n",
    "plt.title(\"Not Sorted\",fontsize=22)\n",
    "\n",
    "plt.figure()\n",
    "mn.matrix(df.sort_values(by=[\"class\",\"age\"]))\n",
    "plt.title(\"Sorted\",fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c354190",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "#### Data Types\n",
    "\n",
    "We have a large dataset with 42 variables. Therefore, we have decided to narrow down to 10 specific data sets that our team finds valuable along wih its corresponding data variable\n",
    "\n",
    "\n",
    "| Variable    | Data Type | Description | \n",
    "| -------- | ------- | ------- |\n",
    "| Age  |  Integer (int64)   | The age of the individual | \n",
    "| Gender |  Categorical (object)(object)    | If they are male or female\n",
    "| Education    |  Categorical (object)   | What level of education they are in: high school, children, some college but no degree, bachelors degree(BA, AB, BS), 7th and 8th grade\n",
    "| Class   |  Categorical (object)  | This is type of of employment of the individual, which includes private, Self-employed-not incorportated, Local government, State government, Self-employed-incorporated, etc\n",
    "| Employment Commitment    |  Categorical (object)  | This is the employment Status of the individual: Children or Amred Forces, Full-time schedules, Not in labor force, PT for non-econ reasons usually FT, Umeployed full-time, \n",
    "| Citizenship    |  Categorical (object)  | What is citizenship status in regards to the United States: Native, Foreign born - Not a citizen of US, Foreign born - US citizen by naturalization, Native - Born abroad of American Parents, Native - Born in Puerto Rice or US Outlying\n",
    "| Wage per hour    |  Integer (int64)  | How much they make per hour\n",
    "| Working_week_per_year   |  Integer (int64)   | How many weeks per year they work (0- 52)\n",
    "| income_above_limit   |  Categorical (object)  | Is the person above or below 50,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f77bab",
   "metadata": {},
   "source": [
    "#### Data Quality\n",
    "\n",
    "Entries chosen that are missing data: Class\n",
    "\n",
    "Many of our columns has missing values. For example, \"Class,\" which we chose to include in our analysis, has only 50% non-null values (104254 out of 209499) meaning there are 105245 values that are null. These quality issues exists because some people don't want to include their class when filling out surveys. We deal with this by eliminating all of these rows that don't have class, because 50% is too much to impute. We could have tried to use nearest neighbor imputation or split-combine-impute, but this would likely skew the results. Many other columns had similar issues, but we decided that they had little impact on the income predictor and so decided to eliminate the entire columns and to choose the 9 most likely to influence the income.\n",
    "\n",
    "We also decided to replace string data for income_above_limit with binary 0s and 1s for better processing with the graphs. Finally, we grouped and consolidated some of the categorical data into smaller bins in order to better understand the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb253bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's clean the dataset a little before moving on\n",
    "\n",
    "# 1. Remove attributes that just arent useful for us\n",
    "for col in ['ID', 'education_institue', 'marital_status', 'is_hispanic', 'unemployment_reason', 'industry_code', 'industry_code_main', 'occupation_code', 'occupation_code_main', 'total_employed', 'household_stat', 'household_summary', 'under_18_family', 'veterans_admin_questionnaire', 'tax_status','gains', 'education_institute', 'employment_stat', 'is_labor_union', 'vet_benefit', 'losses', 'stocks_status', 'mig_year', 'country_of_birth_own',  'country_of_birth_father', 'country_of_birth_mother', 'migration_code_change_in_msa', 'migration_prev_sunbelt', 'migration_code_move_within_reg', 'residence_1_year_ago', 'migration_code_change_in_reg', 'old_residence_reg', 'old_residence_state', 'importance_of_record'  ]:\n",
    "    if col in df:\n",
    "        del df[col]\n",
    "        \n",
    "# Drop class\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# change gender to male - 1, female - 0\n",
    "# df['gender'].replace({0:'Female',1:'Male'},inplace=True)\n",
    "# df['gender'] = np.select(\n",
    "#     [df['gender'].eq(0), df['gender'].eq(1)], ['Female', 'Male'], default=np.nan\n",
    "# )\n",
    "\n",
    "df['gender'].replace((' Male', ' Female'), (1, 0), inplace=True)\n",
    "\n",
    "# change income value to binary\n",
    "df['income_above_limit'].replace(('Above limit', 'Below limit'), (1, 0), inplace=True)\n",
    "# df['education'].replace(('10th grade'), ('test'), inplace=False)\n",
    "df['education'].replace([' 12th grade no diploma', ' 11th grade',' 10th grade',' 9th grade',' 1st 2nd 3rd or 4th grade',' 5th or 6th grade',' Less than 1st grade',' 7th and 8th grade'], 'Less than high school', inplace=True)\n",
    "df['education'].replace([' Some college but no degree'], ' High school graduate', inplace=True)\n",
    "df['education'].replace([' Doctorate degree(PhD EdD)',], 'PHD', inplace=True)\n",
    "df['education'].replace([' Prof school degree (MD DDS DVM LLB JD)',' Masters degree(MA MS MEng MEd MSW MBA)'], 'Master\\'s/Prof', inplace=True)\n",
    "df['education'].replace([' Associates degree-occup /vocational',' Associates degree-academic program'], 'Associates', inplace=True)\n",
    "\n",
    "\n",
    "# print(f\"the value is {df['education'].unique()}\")\n",
    "\n",
    "#('Less than high school', 'Less than high school',\"Less than high school\",\"Less than high school\")\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n",
    "mn.matrix(df)\n",
    "plt.title(\"Not Sorted\",fontsize=22)\n",
    "\n",
    "plt.figure()\n",
    "mn.matrix(df.sort_values(by=[\"class\",\"age\"]))\n",
    "plt.title(\"Sorted\",fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3dedc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c06aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5037259",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "We tried to explore the wage rate trends with respect to education and class at first, but we encountered some discrepancies in the data. which led us to implore ways to correct them. For example, using Wage Rate feature led to all self-employed people having 0 dollars per hour marked down, which isn't very intuitive at first, but further investigation led us to the conclusion that when people weren't paid hourly, they simply put down a 0 which made the graphs considerably skewed.  This made us reconsider using the wage rate for the final graphs without at least first imputing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c24a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "fig = plt.figure(figsize=(30,5))\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "\n",
    "\n",
    "df['age'].plot.hist(bins=40)\n",
    "df['age'].plot.kde(bw_method=0.1, secondary_y=True)\n",
    "plt.xlim(0, 100)\n",
    "plt.title('Age Distribution')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f5ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation matrix \n",
    "plt.subplots(figsize=(10, 8))\n",
    "vars_to_use = ['age', 'working_week_per_year', 'wage_per_hour', 'income_above_limit'] # pick vars\n",
    "plt.pcolor(df[vars_to_use].corr()) # do the feature correlation plot\n",
    "\n",
    "# fill in the indices\n",
    "plt.yticks(np.arange(0.5, len(vars_to_use), 1), vars_to_use)\n",
    "plt.xticks(np.arange(0.5, len(vars_to_use), 1), vars_to_use)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first group the data\n",
    "df_grouped = df.groupby(by=['education'])\n",
    "\n",
    "# tabulate survival rates of each group\n",
    "wage_rate = df_grouped.wage_per_hour.sum() / df_grouped.wage_per_hour.count()\n",
    "\n",
    "# plot the bar chart using builtin pandas API\n",
    "fig, ax = plt.subplots(figsize=(5, 5))  # Increase the figure size to make the plot taller\n",
    "ax = wage_rate.plot(kind='barh')\n",
    "ax.set_title('Wage Rate by Education')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_grouped = df.groupby(by=['class'])\n",
    "\n",
    "# tabulate survival rates of each group\n",
    "wage_rate = df_grouped.wage_per_hour.sum() / df_grouped.wage_per_hour.count()\n",
    "\n",
    "# plot the bar chart using builtin pandas API\n",
    "fig, ax = plt.subplots(figsize=(5, 5))  # Increase the figure size to make the plot taller\n",
    "ax = wage_rate.plot(kind='barh')\n",
    "ax.set_title('Wage Rate by Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5189dbc",
   "metadata": {},
   "source": [
    "## Data Discovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns boxplot\n",
    "plt.subplots(figsize=(30, 10))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(x=\"gender\", y=\"age\", hue=\"income_above_limit\", data=df)\n",
    "plt.title('Boxplot of gender vs age and income above limit')\n",
    "\n",
    "\n",
    "\n",
    "# plt.subplot(1,3,3)\n",
    "# sns.swarmplot(x=\"gender\", y=\"age\", hue=\"income_above_limit\", data=df, s=3) # s controls marker size (like bins or bw)\n",
    "# plt.title('Swarm Example')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(30, 10))\n",
    "plt.subplot(1,3,2)\n",
    "sns.violinplot(x=\"gender\", y=\"age\", hue=\"income_above_limit\", data=df)\n",
    "plt.title('Violin of gender vs age and income above limit')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(60, 10))\n",
    "plt.subplot(1,3,2)\n",
    "sns.violinplot(x=\"education\", y=\"age\", hue=\"income_above_limit\", data=df)\n",
    "plt.title('Violin of education vs age and income above limit')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(60, 10))\n",
    "plt.subplot(1,3,2)\n",
    "sns.violinplot(x=\"citizenship\", y=\"age\", hue=\"income_above_limit\", data=df)\n",
    "plt.title('Violin of citizenship vs age and income above limit')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(60, 10))\n",
    "plt.subplot(1,3,2)\n",
    "sns.violinplot(x=\"employment_commitment\", y=\"age\", hue=\"income_above_limit\", data=df)\n",
    "plt.title('Violin of employment_commitment vs age and income above limit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32891bf",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "How evident is the wage gap between men and women? \n",
    "\n",
    "[Gender Wage Violin Image]\n",
    "\n",
    "We can notice that there are more men than women who fall below the income threshold. By observing that there are more men than women who fall below the income threshold, we are witnessing a significant disparity in income distribution between genders within the dataset. This finding highlights the existence of a potential wage gap between men and women, where women, on average, may be earning less than men.\n",
    "\n",
    "\n",
    "\n",
    "## Question 2:\n",
    "What are the trends of income with education \n",
    "\n",
    "[Education Mutated Violin]\n",
    "\n",
    "It becomes pretty evident from the graph that having at least a Bachelors degree greatly increases your probablity of having an income above the median. Highschool graduates tend to be below the limit during their younger years but a few manage to cross the threshold. Higher levels of education are often associated with access to higher-paying jobs, increased skills and expertise, and greater opportunities for career advancement. This finding highlights the importance of investing in education and lifelong learning as a means of increasing economic opportunities and improving financial outcomes.\n",
    "\n",
    "\n",
    "## Question 3\n",
    "\n",
    "What are the trends of employee commitment\n",
    "\n",
    "From the violin graph, you can notice that if you are unempoloyed, not in the labor force, or unemployed part time, than you tend to be below the limit while those who are employed full time or armed forces tend to be above the limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"gender\", y=\"age\", hue=\"income_above_limit\", data=df,\n",
    "               split=True, # split across violins\n",
    "               inner=\"quart\", # show innner stats like mean, IQR,\n",
    "               scale=\"count\") # scale the size of the plot by the count within each group\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(20, 10))  # Increase the figure size to make the plot wider\n",
    "\n",
    "sns.violinplot(x=\"education\", y=\"age\", hue=\"income_above_limit\", data=df,\n",
    "               split=True,  # split across violins\n",
    "               inner=\"quart\",  # show inner stats like mean, IQR,\n",
    "               scale=\"count\")  # scale the size of the plot by the count within each group\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(20, 10))  # Increase the figure size to make the plot wider\n",
    "sns.violinplot(x=\"citizenship\", y=\"age\", hue=\"income_above_limit\", data=df,\n",
    "               split=True,  # split across violins\n",
    "               inner=\"quart\",  # show inner stats like mean, IQR,\n",
    "               scale=\"count\")  # scale the size of the plot by the count within each group\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(25, 10))  # Increase the figure size to make the plot wider\n",
    "sns.violinplot(x=\"employment_commitment\", y=\"age\", hue=\"income_above_limit\", data=df,\n",
    "               split=True,  # split across violins\n",
    "               inner=\"quart\",  # show inner stats like mean, IQR,\n",
    "               scale=\"count\")  # scale the size of the plot by the count within each group\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869adc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.graph_objs import Scatter, Layout\n",
    "from plotly.graph_objs.scatter import Marker\n",
    "from plotly.graph_objs.layout import XAxis, YAxis\n",
    "import plotly\n",
    "import numpy as np\n",
    "# let's manipulate the example to serve our purposes\n",
    "\n",
    "# limit the dataframe to 10000 values\n",
    "df_limit = df.head(10000)\n",
    "\n",
    "# plotly allows us to create JS graph elements, like a scatter object\n",
    "# we put together a dictionary of plotting elements\n",
    "# 'data': is the key where we place plotting elements\n",
    "plotly.offline.iplot({\n",
    "    'data':[\n",
    "        # plot siblings and spouses versus age as scatter plot\n",
    "        # and make mouseover text for if they survived\n",
    "        # create markers that are sized based on the fare paid\n",
    "        Scatter(x=df_limit.wage_per_hour.values+np.random.rand(*df_limit.wage_per_hour.shape)/2,\n",
    "                y=df_limit.age,\n",
    "                text=df_limit.income_above_limit.values.astype(str),\n",
    "                marker=Marker(size=df_limit.working_week_per_year, sizemode='area', sizeref=1,),\n",
    "                mode='markers')\n",
    "            ],\n",
    "    'layout': Layout(xaxis=XAxis(title='Wage_per_hour'),\n",
    "                     yaxis=YAxis(title='Age'),\n",
    "                     title='Age and Family Size (Marker Size==Fare)')\n",
    "}, show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf18eb8",
   "metadata": {},
   "source": [
    "## Exception Work (With UMAP)\n",
    "We tried to explore the wage rate trends with respect to education and class at first, but we encountered some discrepancies in the data. which led us to implore ways to correct them. For example, using Wage Rate feature led to all self-employed people having 0 dollars per hour marked down, which isn't very intuitive at first, but further investigation led us to the conclusion that when people weren't paid hourly, they simply put down a 0 which made the graphs considerably skewed.  This made us reconsider using the wage rate for the final graphs without at least first imputing data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn numba umap-learn umap-learn[plot]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas matplotlib datashader bokeh holoviews colorcet scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce582e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "\n",
    "# Assuming 'df' is your DataFrame containing the income data\n",
    "\n",
    "# Define selected columns including 'education', 'gender', and 'wage_per_hour'\n",
    "selected_columns = [\"education\", \"gender\", \"wage_per_hour\"]\n",
    "\n",
    "# Clean data to only include selected columns\n",
    "cleaned_data = df[selected_columns]\n",
    "\n",
    "# Encode 'education' column to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "cleaned_data['education'] = label_encoder.fit_transform(cleaned_data['education'])\n",
    "\n",
    "# Standardize the data\n",
    "scaled_data = StandardScaler().fit_transform(cleaned_data)\n",
    "\n",
    "# Reduce dimensionality using UMAP with default parameters\n",
    "umap_model = umap.UMAP()\n",
    "embedding = umap_model.fit_transform(scaled_data)\n",
    "\n",
    "# Define a color mapping based on 'gender' and 'education' columns\n",
    "color_mapping_combined = cleaned_data['gender'] * len(cleaned_data['education'].unique()) + cleaned_data['education']\n",
    "\n",
    "# Define a color palette\n",
    "palette = sns.color_palette(\"tab10\", len(cleaned_data['gender'].unique()) * len(cleaned_data['education'].unique()))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=[palette[x] for x in color_mapping_combined]\n",
    ")\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of the Income Data', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
    "plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.colorbar(label='Gender-Education Combination')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a8805",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "The implementation of logistic regression must be written only from the examples given to you by the instructor. No credit will be assigned to teams that copy implementations from another source, regardless of if the code is properly cited. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "083fd3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Binary Logistic Regression Object, Not Trainable\n"
     ]
    }
   ],
   "source": [
    "# Implementation of logistic regression according to instructor course\n",
    "# Reference: https://github.com/eclarson/MachineLearningNotebooks/blob/master/05.%20Logistic%20Regression.ipynb\n",
    "\n",
    "import numpy as np\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_intercept(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self, X, add_intercept=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_intercept(X) if add_intercept else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "        \n",
    "blr = BinaryLogisticRegressionBase(0.1)\n",
    "print(blr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c81a2",
   "metadata": {},
   "source": [
    "### Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template developed by the instructor in the course. You should add the following functionality to the logistic regression classifier:\n",
    "\n",
    "- Ability to choose optimization technique when class is instantiated: either steepest ascent, stochastic gradient ascent, and Newton's method. It is recommended to call this the \"solver\" input for the class.\n",
    "\n",
    "- Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1 and L2 regularization). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef58b16",
   "metadata": {},
   "source": [
    "###  Train your classifier to achieve good generalization performance. That is, adjust the optimization technique and the value of the regularization term(s) \"C\" to achieve the best performance on your test set. Visualize the performance of the classifier versus the parameters you investigated.\n",
    "\n",
    "\n",
    "Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2452a05",
   "metadata": {},
   "source": [
    "### [1.5 points] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time and classification performance. Discuss the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938de82",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party implementation)? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325f6ee",
   "metadata": {},
   "source": [
    "# Exceptional Work (Required for 7000)\n",
    "\n",
    "\n",
    "Implement an optimization technique for logistic regression using mean square error as your objective function (instead of maximum likelihood). Derive the gradient updates for the Hessian and use Newton's method to update the values of \"w\". Then answer, which process do you prefer: maximum likelihood OR minimum mean-squared error?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
